# $\mathtt{MUST}$: An Effective and Scalable Framework for Multimodal Search with Target Modality
## Introduction

We introduce a new research problem: multimodal search with target modality. This problem involves searching for objects in one modality (the target) using multiple modalities as input. One of the input modalities is the target modality, and the others are the auxiliary modalities. The auxiliary modalities modify or refine some aspects of the target modality input. For example, we can search for videos using a reference video and auxiliary image and text. Our paper entitled "$\mathtt{MUST}$: An Effective and Scalable Framework for Multimodal Search with Target Modality" provides an efficient and scalable framework for **<u>mu</u>**ltimodal **<u>s</u>**earch with **<u>t</u>**arget modality, called $\mathtt{MUST}$. The evaluation results demonstrate that $\mathtt{MUST}$ improves search accuracy by about 50%, is more than 10$\times$ faster than the baseline methods, and can scale to more than 10 million data size.

<img src="/Users/wmz/Library/Application Support/typora-user-images/image-20230413095536726.png" alt="image-20230413095536726" style="zoom:30%;" />

This repo contains the code, dataset, optimal parameters, and other detailed information used for the experiments of our paper.

## Baseline



## Datasets



## Parameters



## Usage

```shell
cd GraphANNS/
git checkout multimodal_search
```

